<!DOCTYPE html>
<html>

  <head>
    <link href="http://s3.amazonaws.com/codecademy-content/courses/ltp/css/shift.css" rel="stylesheet">
    
    <link rel="stylesheet" href="http://s3.amazonaws.com/codecademy-content/courses/ltp/css/bootstrap.css">
    <link rel="stylesheet" href="opinions.css">
    
  </head>
  
    <div class="jumbotron">
        <div class="container">
            <div class="pull-right">
                <h1>Artificial Intelligence</h1>
            </div>
        </div>
    </div> 

  <body>
    <div class="nav">
      <div class="container">
        <ul class="row">
            <div class="col-md-2">
            </div>
            <div class="col-md-10">
                <li><a href="index.html">Home</a></li>
                <li><a href="opinions.html">Opinions</a></li>
                <li><a href="timeline.html">Timeline</a></li>
                <li><a href="resources.html">Resources</a></li>
            </div>
        </ul>
      </div>
    </div>

    <div class="message">
        <div class="container">
            <h2>Question 2: Are the Potential Consequences of Artificial Intelligence Mostly Positive or Negative?</h2>
            <h3>Your answer: The consequences are mostly positive.</h3>
	    <p>You have said that the consequences are mostly positive. You probably see AI as something that can improve our lives and make them easier.</p>
	    <p>To analyze this, we are going to look at AI as a medium and analyze it through Marshall McLuhan’s 4 Laws of Media. They are that any new form of media will:</p>
	    <li>enhance a basic human function</li>
	    <li>obsolesce an old medium</li>
	    <li>retrieve an old function</li>
	    <li>reverse or do the opposite of its function in some way</li>
	    <p> Someone opposed to your view would probably focus on what AI obsolesces and reverses, as these are typically the more negative aspects of a new medium.</p>
	    <p>Applying McLuhan’s second law of media (“what will this obsolesce?”), there will be something that weak AI will render obsolete. While weak AI is not as potent in this regard as strong AI, McLuhan’s law of media will still be applicable. For many computerized board or card games (and some video games), weak AI has been implemented to compensate for a lack of human interaction. Pushed further, there does not need to be any human interaction if we make the human “opponent” or “other player” obsolete. Another example is the Roomba. In principle, we could make the broom and dustpan obsolete by relying on the Roomba (or similar technologies) to clean our floors should they get dirty. There are larger scale impacts of weak AI in terms of McLuhan’s law of obsolescence. A notable example of this would be the idea of the automated factory. Here, many human workers are rendered obsolete because computers can perform the same kinds of tasks in a more efficient manner. These examples highlight weak AI’s ability to obsolesce different kinds of tasks that would otherwise have needed humans to accomplish. While weak AI’s ability to improve efficiency and convenience is noted, its tendency to obsolesce previously human activities will lead to some negative consequences.</p>
	    <p>By McLuhan’s laws of media, a strong AI would also obsolesce something. Strong AI is far more dangerous than weak AI in this regard. There is the potential to render all of humankind obsolete with the creation of strong AI. An intelligence explosion, also referred to as the singularity, would cause this. The singularity is the concept that once a machine reaches human level intelligence, it will surpass humans very rapidly. As a machine becomes more intelligent than a human, it is better at designing other intelligent machines. The machine that the first machine creates will also be capable of creating a more intelligent machine. As this cycle continues, ever more intelligent machines are created. This leaves humans incapable of even imagining the machine’s intelligence. Many authors and scientists, such as Marcus Hutter, David Chalmers and <a href=”http://goertzel.org/TenYearsToTheSingularity.pdf“>Ben Goertzel</a> agree that this intelligence explosion is not only possible but also likely.</p>
	    <p>Chalmers gives four outcomes of the singularity for humans: extinction, isolation, inferiority, or integration. The first three options would obsolesce human beings in a sense. The first, extinction is obvious as humans would be obsolete if we no longer exist. The second option, isolation, would mean that humans would exist without interaction with the AI; with either AI or humans existing in a separate virtual world. In the case where AI occupies a separate virtual world humans would not become obsolete but all technology would be obsolete to humans since we could no longer use it, in a sense a part of us, our “virtual self” would become obsolete. In the case where it is us forced to live in the virtual world, humans as they have always been, would become obsolete. There would no longer be a place for human beings in the physical world and so the physical body would become useless. Many of the things that occupy our time and thought would become pointless, such as providing for our basic needs. The third option, inferiority, is similar in this regard. With human beings as inferiors to AI there would no longer be a need to concentrate on the same things. Our world would be controlled and governed by AI, making human intelligence obsolete. The fourth option is the only option where humans do not become obsolete, but enhanced to integrate with AI. Therefore, it is likely that strong AI would obsolesce humans in some way and if we want to avoid this, steps must be taken to assure integration is realized.</p>
	    <p>The obsolescence caused by weak and strong AI are similar in the regard that they are replacing older forms of media that were also used to perform tasks which would usually require considerable thought and precision if they were done by humans. The difference is how weak AI is very specific in obsolescing old media or human mental functions, but strong AI has the potential to obsolesce all of humanity’s mental functions. A strong AI could also function similarly to a network of weak AI, which would obsolesce all of the existing weak AI media. It would extremely efficient, and a good comparison would be how smartphones have combined so many other important media such as the telephone, desktop computer, calculator, television, radio, camera, data storage, clock, etc. The way in which strong AI could obsolesce humans is actually more related to the consequences of strong AI reversal.</p>
	    <p>The media law of retrieval is related to the idea of obsolescence. The question of retrieval is “what previous action was obsoleted by an older media, but has called back into action by the new media?”. Due to the broad application of weak AI, this retrieval mechanism will only be evaluated for some individual examples (but a more encompassing theme of retrieval will be done when evaluating strong AI, in the paragraph below). Since weak AI is not as cognitively powerful as strong AI, weak AI has some limitations on retrieval that strong AI does not. Primarily, these limitations are due to the need to tailor the weak AI to a particular application, whereas strong AI may be able to adapt itself to the new application. Consequently, the capacity for retrieval of weak AI has been limited to applications such as an ‘AI Player’ in board games that were (to some extent) made ‘obsolete’ by the development of computer games that are widely used today.</p>
	    <p>The adoption of new technology can go unquestioned, as people can see the enhancements and benefits of new a media, but fail to foresee the retrieval or reversal mechanisms. As computer processing power has been replacing the old ways of doing science, people have exemplified a religious faith in the power of computers. In the Age of Reason and with the development of the scientific method, people were turning away from religion as a means to understand the world, and began to trust empirical science. Joseph Weizenbaum was a professor at MIT and developed one of the first convincing programs to emulate human conversation, ELIZA. This was a great example of weak AI, and the design was to run a script called Doctor, which emulated a session with a psychotherapist. He was shocked by the trust ELIZA’s users and his students placed in science. He describes some of his students as having “already rejected all ways but the scientific to come to know the world, and who only seek a deeper, more dogmatic indoctrination in that faith (although that word is no longer in their vocabulary)”. He continues later with a quote from Hannah Arendt in regards to Pentagon operations “(they) did not judge; they calculated… an utterly irrational confidence in the calculability of reality (became) the (theme) of the decision making”. Believing purely in the scientific method to explain the way in which the world worked replaced the blind faith of religion. However, modern science is not as simple as the old scientific method with observation and concrete evidence. Much of the scientific progress today uses computer modeling, statistics, and programs that were not designed by the scientists which use them. How computers truly function is understood by few, but trusted by many. Computers have effectively retrieved the blind faith demonstrated in religion. Despite the lack of understanding how computers work, they continue to be integrated into society. Langdon Winner describes the idea of “mythinformation”, which is “the almost religious conviction that a widespread adoption of computers and communications systems along with easy access to electronic information will automatically produce a better world for human living”. So people continue to trust computers and weak AI, but what level of trust and acceptance would they have for a strong AI? The seemingly miraculous enhancements and endless uses of a strong AI would certainly garner a very high level of support. A true strong AI would be working on a thinking level above humans, and its power could be perceived as almost godly. Strong AI could retrieve a time similar to that at the height of the Roman Empire and the Catholic Church. The supercomputer AI would be similar to the Pope, as an enlightened being who is closest in touch to fundamental power of the universe (which in modern times would be logic and math, and in the past it was the Christian view of God). Strong AI could see the return of a pyramid shaped way of thinking with strong AI at the peak providing top down control, instead of the distribution of thinking power provided by the scientific method or ideally an equality of power through access to information (“mythinformation”).</p>
	    <p>Next is McLuhan’s law of reversal. This will be the scenario where weak AI’s development and/or deployment can result in negative consequences if taken to an extreme. When looking at Weak AI, it is also important to address some of its limitations including how overuse could cause unwanted consequences. One such limitation is that it is generally not self-learning or self-adaptive. While this does alleviate the potential for negative consequences of <a href=”http://en.wikipedia.org/wiki/Recursive_self-improvement”>recursive self-improvement</a>, this causes weak AI programs to only be as good as we can program them. The issue here is the potential for an artificial intelligence to fail because we did not program it correctly. This is a serious issue that is already known to cause some problems. Take ‘Knight Capital Group’ as an example: they used an automated piece of software (weak AI) to automate the company’s trading. Long story short, there was a programming error that resulted in Knight Capital losing about $440 million. Bringing this into a larger perspective, weak AI usage is very widespread and we are dependent on it to function correctly. Modern computers are performing calculations at an ever increasing rate, so it becomes difficult to detect potential errors. This is especially true as these weak AI systems are becoming less dependent on human intervention. Therefore, one can see that less human involvement coupled with increasing use of such systems would be a volatile mix that could bring McLuhan’s law of reversal into effect.</p>
	    <p>It can be speculated that, each purpose for strong AI, could in practice have the opposite of the intended effect, following McLuhan’s law of reversal. While in general AI is intended to make life easier, it is a very real probability that it will make the life of the average human much more difficult as strong AI would cause widespread unemployment. Machines having human level intelligence with computing power possibly far exceeding our own would leave very few jobs for human workers. Some AI is being designed as social robots; this could lead to far less interaction between human beings. We have already seen a decrease in real life interaction since the emergence of cell phones and the internet. Imagine what effect having a robot companion who completely understands all the needs and motivations of its owner could have on the need for basic relationships and social interactions of human beings. Also, as we begin to depend on AI to do all of our thinking and problem solving, perhaps our own intelligence will decline.</p>
	    <p>Comparing the tendency of weak and strong AI to backfire, we can consider strong AI to be far more dangerous than weak AI because of its far greater capabilities. Everything that AI can accomplish could, in some way, lead to negative consequences. These consequences would be greater for strong AI because it has the potential to replace everything that humans can do, whereas weak AI can only replace the more repetitive tasks. As Ray Kurzweil writes, there is the sentiment (‘the pessimistic view’) that the advent of strong AI may lead to the end of the human race. What is interesting about strong AI and its reversibility (compared to weak AI), is that its potential capabilities are so extreme, that even the other laws of media can be taken as reversibility. Strong AI could enhance everything that humans can do, which could obsolesce human activities, which would be its reversibility. This can also be considered as the devaluation of humanity. Weizenbaum writes “I argue... that there are some acts of thought that ought to be attempted only by humans”. This follows the idea that some people believe that strong AI should not be bounded in its capabilities, while Weizenbaum believes that people may be alienated by machines because of a lack of emotional interaction between humans. To prevent these kinds of consequences, Kurzweil suggests that we attempt to “continue the advance our social ideals” to avoid potentially destructive outcomes. Kurzweil argues that our advancement of social ideals has already decreased violence in the world and has even aided a conversation about the ethics of biotechnology. To mitigate the potential negative outcomes of AI (especially strong AI), more ethical conversations are needed. While weak AI does have some potential negative consequences, they are not as powerful as those posed by strong AI. Therefore, the allure of potential benefits of strong AI should be approached with caution because they can easily reverse into potential problems.</p>
        </div>
    </div>

    <div class="questions">
	    <div class="container">
	      <h3>Question 3: Should We Pursue Artificial Intelligence?</h3>
			  <p><a href="question3a.html">a) Yes, the benefits for the human race outweigh the risks.</a></p>
			  <p><a href="question3b.html">b) No, the risks are too high.</a></a></p>
	    </div>
    </div>
	
    <div class="nav">
      <div class="container">
        <ul class="row">
            <div class="col-md-2">
            </div>
            <div class="col-md-10">
                <li><a href="index.html">Home</a></li>
                <li><a href="opinions.html">Opinions</a></li>
                <li><a href="timeline.html">Timeline</a></li>
                <li><a href="resources.html">Resources</a></li>
            </div>
        </ul>
      </div>
    </div>
  </body>
</html>
